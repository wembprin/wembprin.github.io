---
---
@inproceedings{pedrotti-etal-2025-humans,
    abbr={ACL},
    title = "How Humans and {LLM}s Organize Conceptual Knowledge: Exploring Subordinate Categories in {I}talian",
    author = "Pedrotti, Andrea  and
      Rambelli, Giulia  and
      Villani, Caterina  and
      Bolognesi, Marianna",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.224/",
    doi = "10.18653/v1/2025.acl-long.224",
    pages = "4464--4482",
    ISBN = "979-8-89176-251-0",
    abstract = "People can categorize the same entity at multiple taxonomic levels, such as basic (bear), superordinate (animal), and subordinate (grizzly bear). While prior research has focused on basic-level categories, this study is the first attempt to examine the organization of categories by analyzing exemplars produced at the subordinate level. We present a new Italian psycholinguistic dataset of human-generated exemplars for 187 concrete words. We then leverage these data to evaluate whether textual and vision LLMs produce meaningful exemplars that align with human category organization across three key tasks: exemplar generation, category induction, and typicality judgment. Our findings show a low alignment between humans and LLMs, consistent with previous studies. However, their performance varies notably across different semantic domains. Ultimately, this study highlights both the promises and the constraints of using AI-generated exemplars to support psychological and linguistic research.",
    selected={true},
    preview={concept-ped.png}
}

@inproceedings{pedrotti-etal-2025-stress,
    abbr={ACL},
    title = "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors",
    author = "Pedrotti, Andrea  and
      Papucci, Michele  and
      Ciaccio, Cristiano  and
      Miaschi, Alessio  and
      Puccetti, Giovanni  and
      Dell{'}Orletta, Felice  and
      Esuli, Andrea",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.156/",
    doi = "10.18653/v1/2025.findings-acl.156",
    pages = "3010--3031",
    ISBN = "979-8-89176-256-5",
    abstract = "Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we evaluate the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. We develop a pipeline that fine-tunes language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT), obtaining generations more challenging to detect by current models. Additionally, we analyze the linguistic shifts induced by the alignment and how detectors rely on ``linguistic shortcuts'' to detect texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detecting performances. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts. We release code, models, and data to support future research on more robust MGT detection benchmarks.",
    preview={mgt-pedrotti.png}
}

@inproceedings{puccetti-etal-2024-abricot,
    abbr={CLiC-it},
    title = "{ABRICOT} - {AB}st{R}actness and Inclusiveness in {CO}ntex{T}: A {CALAMITA} Challenge",
    author = "Puccetti, Giovanni  and
      Collacciani, Claudia  and
      Ravelli, Andrea Amelio  and
      Esuli, Andrea  and
      Bolognesi, Marianna",
    editor = "Dell'Orletta, Felice  and
      Lenci, Alessandro  and
      Montemagni, Simonetta  and
      Sprugnoli, Rachele",
    booktitle = "Proceedings of the Tenth Italian Conference on Computational Linguistics (CLiC-it 2024)",
    month = dec,
    year = "2024",
    address = "Pisa, Italy",
    publisher = "CEUR Workshop Proceedings",
    html = "https://aclanthology.org/2024.clicit-1.128/",
    pages = "1161--1167",
    ISBN = "979-12-210-7060-6",
    abstract = "The ABRICOT Task is designed to evaluate Italian language models on their ability to understand and assess the abstractness and inclusiveness of language, two nuanced features that humans naturally convey in everyday communication. Unlike binary categorizations such as abstract/concrete or inclusive/exclusive, these features exist on a continuous spectrum with varying degrees of intensity. The task is based on a manual collection of sentences that present the same noun phrase (NP) in different contexts, allowing its interpretation to vary between the extremes of abstractness and inclusiveness. This challenge aims to verify the how LLMs perceive subtle linguistic variations and their implications in natural language.",
    preview={abricot-puccetti.png}
}

@inproceedings{moroni-etal-2025-optimizing,
    abbr={NAACL},
    title = "Optimizing {LLM}s for {I}talian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation",
    author = "Moroni, Luca  and
      Puccetti, Giovanni  and
      Huguet Cabot, Pere-Llu{\'i}s  and
      Bejgu, Andrei Stefan  and
      Miaschi, Alessio  and
      Barba, Edoardo  and
      Dell{'}Orletta, Felice  and
      Esuli, Andrea  and
      Navigli, Roberto",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2025",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-naacl.371/",
    doi = "10.18653/v1/2025.findings-naacl.371",
    pages = "6646--6660",
    ISBN = "979-8-89176-195-7",
    abstract = "The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token ``fertility'') and slower inference speed.In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7B-v0.1, reducing token fertility by 25{\%}, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.",
    preview={opt-puccetti.png}
}

@inproceedings{puccetti-etal-2025-wordnet,
    abbr={GWC},
    title = "{W}ordnet and Word Ladders: Climbing the abstraction taxonomy with {LLM}s",
    author = "Puccetti, Giovanni  and
      Esuli, Andrea  and
      Bolognesi, Marianna",
    editor = "Zanchi, Chiara  and
      Brigada Villa, Luca  and
      Biagetti, Erica  and
      Rademaker, Alexandre  and
      Bond, Francis  and
      Rigau, German",
    booktitle = "Proceedings of the 13th Global Wordnet Conference",
    month = jan,
    year = "2025",
    address = "Pavia, Italy",
    publisher = "Global Wordnet Association",
    url = "https://aclanthology.org/2025.gwc-1.7/",
    doi = "10.18653/v1/2025.gwc-1.7",
    pages = "51--65",
    abstract = "WordNet has long served as a benchmark for approximating the mechanisms of semantic categorization in the human mind, particularly through its hierarchical structure of word synsets, most notably the IS-A relation. However, these semantic relations have traditionally been curated manually by expert lexicographers, relying on external resources like dic- tionaries and corpora. In this paper, we explore whether large language models (LLMs) can be leveraged to approximate these hierarchical semantic relations, potentially offering a scalable and more dynamic alternative for maintaining and updating the WordNet taxonomy. This investigation addresses the feasibility and implications of automating this process with LLMs by testing a set of prompts encoding dif- ferent sociodemographic traits and finds that adding age and job information to the prompt affects the model ability to generate text in agreement with hierarchical semantic relations while gender does not have a statistically significant impact.",
    selected={true},
    preview={wordladders-pucc.png}
}

@inproceedings{puccetti-etal-2025-invalsi,
    abbr={COLING},
    title = "The Invalsi Benchmarks: measuring the Linguistic and Mathematical understanding of Large Language Models in {I}talian",
    author = "Puccetti, Giovanni  and
      Cassese, Maria  and
      Esuli, Andrea",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2025.coling-main.453/",
    pages = "6782--6797",
    abstract = "While Italian is a high-resource language, there are few Italian-native benchmarks to evaluate generative Large Language Models (LLMs) in this language. This work presents three new benchmarks: Invalsi MATE to evaluate models performance on mathematical understanding in Italian, Invalsi ITA to evaluate language under standing in Italian and Olimpiadi MATE for more complex mathematical understanding. The first two benchmarks are based on the Invalsi tests, which are administered to students of age between 6 and 18 within the Italian school system and have been validated by several experts in teaching and pedagogy, the third one comes from the Italian highschool math Olympics. We evaluate 10 powerful language models on these benchmarks and we find that they are bound by 71{\%} accuracy on Invalsi MATE, achieved by Llama 3.1 70b instruct and by 88{\%} on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we compare LLMs with the average performance of Italian students to show that Llama 3.1 is the only one to outperform them on Invalsi MATE while most models do so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than Invalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct accuracy is 45{\%}.",
    preview={invalsi-puccetti.png}
}

@inproceedings{sperduti-nguyen-2025-pset,
    abbr={EMNLP},
    title = "{PSET}: a Phonetics-Semantics Evaluation Testbed",
    author = "Sperduti, Gianluca  and
      Nguyen, Dong",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.373/",
    doi = "10.18653/v1/2025.emnlp-main.373",
    pages = "7346--7356",
    ISBN = "979-8-89176-332-6",
    abstract = "We introduce the Phonetics-Semantics Evaluation Testbed (PSET), a new English-based testbed to evaluate phonetic embeddings. Our testbed is built on the assumption that phonetic embeddings should always prioritize phonetics over semantics, and it therefore leverages homophones and synonyms.We use PSET to test three phonetic embedding models: articulatory embeddings, Phoneme2Vec, and XPhoneBERT. The phonetic-based embeddings solve the task with varying degrees of success, with Phoneme2Vec performing the best.We also test five recent LLMs, GPT-4o, Gemini 2.5 Flash, Llama 3.1-8B, OLMo-7B and OLMo 2-7B. Gemini 2.5 Flash performs better than the other models. With this testbed, we hope to advance the development and evaluation of phonetic embedding models.",
    selected={true},
    preview={pset-sperduti.png}
}

@inproceedings{loia-etal-2025-evaluating,
  abbr={under review},
  title = "Evaluating misspelling patterns in gamified data: A sociodemographic analysis for linguistic research",
  author = "Loia, Adele and Sperduti, Gianluca and Bolognesi, Marianna",
  year = "2026",
  booktitle = "under review",
  }

@article{sperduti2025misspellingsnaturallanguageprocessing,
      abbr={NLPJ},
      title={Misspellings in Natural Language Processing: A survey},
      journal={Natural Language Processing Journal},
      author={Gianluca Sperduti and Alejandro Moreo},
      year={2026},
      eprint={2501.16836},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      html={https://arxiv.org/abs/2501.16836}, 
      preview={misspellings-sperduti.png},
      abstract={This survey provides an overview of the challenges of misspellings in natural language processing (NLP). While often unintentional, misspellings have become ubiquitous in digital communication, especially with the proliferation of Web 2.0, user-generated content, and informal text mediums such as social media, blogs, and forums. Even if humans can generally interpret misspelled text, NLP models frequently struggle to handle it: this causes a decline in performance in common tasks like text classification and machine translation. In this paper, we reconstruct a history of misspellings as a scientific problem. We then discuss the latest advancements to address the challenge of misspellings in NLP. Main strategies to mitigate the effect of misspellings include data augmentation, double step, character-order agnostic, and tuple-based methods, among others. This survey also examines dedicated data challenges and competitions to spur progress in the field. Critical safety and ethical concerns are also examined, for example, the voluntary use of misspellings to inject malicious messages and hate speech on social networks. Furthermore, the survey explores psycholinguistic perspectives on how humans process misspellings, potentially informing innovative computational techniques for text normalization and representation. Finally, the misspelling-related challenges and opportunities associated with modern large language models are also analyzed, including benchmarks, datasets, and performances of the most prominent language models against misspellings. This survey aims to be an exhaustive resource for researchers seeking to mitigate the impact of misspellings in the rapidly evolving landscape of NLP.}
}

@misc{sperduti2025typoglycemiahoodinvestigatinglanguage,
      abbr={under review},
      title={Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words}, 
      author={Gianluca Sperduti and Alejandro Moreo},
      year={2025},
      eprint={2510.21326},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      html={https://arxiv.org/abs/2510.21326}, 
      preview={typo-sperduti.png},
      abstract={Research in linguistics has shown that humans can read words with internally scrambled letters, a phenomenon recently dubbed typoglycemia. Some specific NLP models have recently been proposed that similarly demonstrate robustness to such distortions by ignoring the internal order of characters by design. This raises a fundamental question: how can models perform well when many distinct words (e.g., form and from) collapse into identical representations under typoglycemia? Our work, focusing exclusively on the English language, seeks to shed light on the underlying aspects responsible for this robustness. We hypothesize that the main reasons have to do with the fact that (i) relatively few English words collapse under typoglycemia, and that (ii) collapsed words tend to occur in contexts so distinct that disambiguation becomes trivial. In our analysis, we (i) analyze the British National Corpus to quantify word collapse and ambiguity under typoglycemia, (ii) evaluate BERT's ability to disambiguate collapsing forms, and (iii) conduct a probing experiment by comparing variants of BERT trained from scratch on clean versus typoglycemic Wikipedia text; our results reveal that the performance degradation caused by scrambling is smaller than expected.}
}